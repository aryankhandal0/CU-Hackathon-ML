{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.ensemble as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UsageClass</th>\n",
       "      <th>CheckoutType</th>\n",
       "      <th>CheckoutYear</th>\n",
       "      <th>CheckoutMonth</th>\n",
       "      <th>Checkouts</th>\n",
       "      <th>Title</th>\n",
       "      <th>Creator</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>PublicationYear</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Tidal wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tsunamis, Tsunamis Juvenile literature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>London holiday / Richard Peck.</td>\n",
       "      <td>Peck, Richard, 1934-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Viking,</td>\n",
       "      <td>1998.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Cinco de Mayo : celebrating Hispanic pride / C...</td>\n",
       "      <td>Gnojewski, Carol</td>\n",
       "      <td>Cinco de Mayo Mexican holiday History Juvenile...</td>\n",
       "      <td>Enslow Publishers,</td>\n",
       "      <td>c2002.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>War stories, Historical fiction, Domestic fict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>As a man thinketh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thought and thinking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID UsageClass CheckoutType  CheckoutYear  CheckoutMonth  Checkouts  \\\n",
       "0   1   Physical      Horizon          2005              4          1   \n",
       "1   2   Physical      Horizon          2005              4          1   \n",
       "2   3   Physical      Horizon          2005              4          3   \n",
       "3   4   Physical      Horizon          2005              4          1   \n",
       "4   5   Physical      Horizon          2005              4          1   \n",
       "\n",
       "                                               Title               Creator  \\\n",
       "0                                         Tidal wave                   NaN   \n",
       "1                     London holiday / Richard Peck.  Peck, Richard, 1934-   \n",
       "2  Cinco de Mayo : celebrating Hispanic pride / C...      Gnojewski, Carol   \n",
       "3                                          Annapolis                   NaN   \n",
       "4                                  As a man thinketh                   NaN   \n",
       "\n",
       "                                            Subjects           Publisher  \\\n",
       "0             Tsunamis, Tsunamis Juvenile literature                 NaN   \n",
       "1                                                NaN             Viking,   \n",
       "2  Cinco de Mayo Mexican holiday History Juvenile...  Enslow Publishers,   \n",
       "3  War stories, Historical fiction, Domestic fict...                 NaN   \n",
       "4                               Thought and thinking                 NaN   \n",
       "\n",
       "  PublicationYear MaterialType  \n",
       "0             NaN         BOOK  \n",
       "1           1998.         BOOK  \n",
       "2          c2002.         BOOK  \n",
       "3             NaN         BOOK  \n",
       "4             NaN         BOOK  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "     ('Title', TfidfVectorizer()),\n",
    "     ('Checkouts', None),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-63e7889a7afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MaterialType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn_pandas/dataframe_mapper.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0my\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0mto\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \"\"\"\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn_pandas/dataframe_mapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, y, do_fit)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# with sparse=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = mapper.fit_transform(df)\n",
    "categories = df['MaterialType']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "categories=le.fit_transform(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(features,categories,test_size=0.2,train_size=0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khandalaryan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sk.RandomForestClassifier(random_state=0)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BOOK       0.80      0.87      0.84      4376\n",
      "          CR       0.50      0.10      0.17        20\n",
      "       MIXED       1.00      0.84      0.91        76\n",
      "       MUSIC       0.71      0.12      0.21        41\n",
      "   SOUNDCASS       0.17      0.04      0.06       194\n",
      "   SOUNDDISC       0.43      0.54      0.48       791\n",
      "   VIDEOCASS       0.26      0.11      0.16       530\n",
      "   VIDEODISC       0.47      0.34      0.40       303\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      6331\n",
      "   macro avg       0.54      0.37      0.40      6331\n",
      "weighted avg       0.68      0.71      0.68      6331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(x_test)\n",
    "print(classification_reportcation_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9960917144346013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test,predicted)\n",
    "acc=(mat[0][0]+mat[1][1])/(mat[0][0]+mat[1][1]+mat[0][1]+mat[1][0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ID','CheckoutType','CheckoutYear','CheckoutMonth','Creator','Subjects','Publisher','PublicationYear','CheckoutType'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    '''\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "bow_transformer = TfidfVectorizer(analyzer=text_process).fit(x)\n",
    "X = bow_transformer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['MaterialType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31653x39279 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 165155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 7, 0, 2])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitpred(nb):\n",
    "    nb.fit(X_train, y_train)\n",
    "    preds = nb.predict(X_test)\n",
    "    mat = confusion_matrix(y_test, preds)\n",
    "    print(mat)\n",
    "    acc=(mat[0][0]+mat[1][1])/(mat[0][0]+mat[1][1]+mat[0][1]+mat[1][0])\n",
    "    print(acc)\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6576    0    0    0    0    3    0    0]\n",
      " [  25    0    0    0    0    0    0    0]\n",
      " [ 104    0    0    0    0    2    0    0]\n",
      " [  42    0    0    0    0   11    0    0]\n",
      " [ 323    0    0    0    0    2    0    0]\n",
      " [ 889    0    0    0    0  299    0    0]\n",
      " [ 800    0    0    0    0    3    0    0]\n",
      " [ 375    0    0    0    0    1    0   41]]\n",
      "0.9962126950462051\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84      6579\n",
      "           1       0.00      0.00      0.00        25\n",
      "           2       0.00      0.00      0.00       106\n",
      "           3       0.00      0.00      0.00        53\n",
      "           4       0.00      0.00      0.00       325\n",
      "           5       0.93      0.25      0.40      1188\n",
      "           6       0.00      0.00      0.00       803\n",
      "           7       1.00      0.10      0.18       417\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      9496\n",
      "   macro avg       0.33      0.17      0.18      9496\n",
      "weighted avg       0.66      0.73      0.64      9496\n",
      "\n",
      "[[6532    0    0    0    0   31   13    3]\n",
      " [  25    0    0    0    0    0    0    0]\n",
      " [  95    0    0    0    0   11    0    0]\n",
      " [  24    0    0    0    0   29    0    0]\n",
      " [ 322    0    0    0    0    3    0    0]\n",
      " [ 838    0    0    0    0  350    0    0]\n",
      " [ 800    0    0    0    0    1    2    0]\n",
      " [ 318    0    0    0    0    5    4   90]]\n",
      "0.9961872807686442\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84      6579\n",
      "           1       0.00      0.00      0.00        25\n",
      "           2       0.00      0.00      0.00       106\n",
      "           3       0.00      0.00      0.00        53\n",
      "           4       0.00      0.00      0.00       325\n",
      "           5       0.81      0.29      0.43      1188\n",
      "           6       0.11      0.00      0.00       803\n",
      "           7       0.97      0.22      0.35       417\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      9496\n",
      "   macro avg       0.33      0.19      0.20      9496\n",
      "weighted avg       0.66      0.73      0.65      9496\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khandalaryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/khandalaryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6579    0    0    0    0    0    0    0]\n",
      " [  25    0    0    0    0    0    0    0]\n",
      " [ 106    0    0    0    0    0    0    0]\n",
      " [  53    0    0    0    0    0    0    0]\n",
      " [ 325    0    0    0    0    0    0    0]\n",
      " [1151    0    0    0    0   37    0    0]\n",
      " [ 803    0    0    0    0    0    0    0]\n",
      " [ 417    0    0    0    0    0    0    0]]\n",
      "0.9962144155057541\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82      6579\n",
      "           1       0.00      0.00      0.00        25\n",
      "           2       0.00      0.00      0.00       106\n",
      "           3       0.00      0.00      0.00        53\n",
      "           4       0.00      0.00      0.00       325\n",
      "           5       1.00      0.03      0.06      1188\n",
      "           6       0.00      0.00      0.00       803\n",
      "           7       0.00      0.00      0.00       417\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      9496\n",
      "   macro avg       0.21      0.13      0.11      9496\n",
      "weighted avg       0.61      0.70      0.58      9496\n",
      "\n",
      "[[6562    0    0    5    1    8    2    1]\n",
      " [  24    1    0    0    0    0    0    0]\n",
      " [  19    0   87    0    0    0    0    0]\n",
      " [  11    0    0   37    0    5    0    0]\n",
      " [ 314    0    0    0    5    6    0    0]\n",
      " [ 797    0    0    3    0  387    0    1]\n",
      " [ 788    0    0    0    1    2    3    9]\n",
      " [ 299    0    0    0    0    1    0  117]]\n",
      "0.9963564596933353\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85      6579\n",
      "           1       1.00      0.04      0.08        25\n",
      "           2       1.00      0.82      0.90       106\n",
      "           3       0.82      0.70      0.76        53\n",
      "           4       0.71      0.02      0.03       325\n",
      "           5       0.95      0.33      0.48      1188\n",
      "           6       0.60      0.00      0.01       803\n",
      "           7       0.91      0.28      0.43       417\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      9496\n",
      "   macro avg       0.84      0.40      0.44      9496\n",
      "weighted avg       0.77      0.76      0.69      9496\n",
      "\n",
      "[[4940    7    3    9   83 1150  297   90]\n",
      " [  12    2    0    0    1    7    3    0]\n",
      " [  13    0   88    1    1    1    2    0]\n",
      " [   2    0    0   38    0   11    2    0]\n",
      " [ 172    1    0    0   20   98   23   11]\n",
      " [ 291    0    0    7   16  807   46   21]\n",
      " [ 363    1    1    0   20  254  127   37]\n",
      " [ 146    0    0    1    1   86   46  137]]\n",
      "0.9961701269905261\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      6579\n",
      "           1       0.18      0.08      0.11        25\n",
      "           2       0.96      0.83      0.89       106\n",
      "           3       0.68      0.72      0.70        53\n",
      "           4       0.14      0.06      0.09       325\n",
      "           5       0.33      0.68      0.45      1188\n",
      "           6       0.23      0.16      0.19       803\n",
      "           7       0.46      0.33      0.38       417\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      9496\n",
      "   macro avg       0.48      0.45      0.45      9496\n",
      "weighted avg       0.68      0.65      0.65      9496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "nb = MultinomialNB()\n",
    "fitpred(nb)\n",
    "nb2 = BernoulliNB()\n",
    "fitpred(nb2)\n",
    "clf = svm.SVC(gamma='scale')\n",
    "fitpred(clf)\n",
    "clf2 = xgb.XGBClassifier()\n",
    "fitpred(clf2)\n",
    "clf3 = DecisionTreeClassifier()\n",
    "fitpred(clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbf = xgb.XGBClassifier()\n",
    "nbf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = test['Title']\n",
    "X_t = bow_transformer.transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nbf.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = le.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOK', 'BOOK', 'SOUNDDISC', ..., 'BOOK', 'BOOK', 'SOUNDDISC'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31654</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31655</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31656</td>\n",
       "      <td>SOUNDDISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31657</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31658</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31659</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31660</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31661</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31662</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31663</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31664</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31665</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31666</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31667</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31668</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31669</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31670</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31671</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31672</td>\n",
       "      <td>SOUNDDISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31673</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31674</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31675</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31676</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31677</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31678</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID MaterialType\n",
       "0   31654         BOOK\n",
       "1   31655         BOOK\n",
       "2   31656    SOUNDDISC\n",
       "3   31657         BOOK\n",
       "4   31658         BOOK\n",
       "5   31659         BOOK\n",
       "6   31660         BOOK\n",
       "7   31661         BOOK\n",
       "8   31662         BOOK\n",
       "9   31663         BOOK\n",
       "10  31664         BOOK\n",
       "11  31665         BOOK\n",
       "12  31666         BOOK\n",
       "13  31667         BOOK\n",
       "14  31668         BOOK\n",
       "15  31669         BOOK\n",
       "16  31670         BOOK\n",
       "17  31671         BOOK\n",
       "18  31672    SOUNDDISC\n",
       "19  31673         BOOK\n",
       "20  31674         BOOK\n",
       "21  31675         BOOK\n",
       "22  31676         BOOK\n",
       "23  31677         BOOK\n",
       "24  31678         BOOK"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = test['ID']\n",
    "label = preds\n",
    "data = { 'ID': id_, 'MaterialType': label}\n",
    "submission = pd.DataFrame(data)\n",
    "submission.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
